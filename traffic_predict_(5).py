# -*- coding: utf-8 -*-
"""traffic-predict (5).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pJkuvmgaOypEEQVMyqJtFJvqCp982kyL

### Traffic Volume Prediction

### Import Libraries
"""

# Required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.utils import resample
import lightgbm as lgb
from sklearn.metrics import accuracy_score, classification_report

"""### Loading Dataset and Exploration"""

# Load dataset
data = pd.read_csv('Train_traffic.csv')

# Display basic information
print(data.info())
print(data.head())

"""### Data Cleaning"""

# Strip column gaps
data.columns = data.columns.str.strip()

# Convert date_time to datetime
data['date_time'] = pd.to_datetime(data['date_time'])

# Handle missing values
data.fillna(method='ffill', inplace=True)

# Remove duplicate rows
data.drop_duplicates(inplace=True)

# Apply the mapping to simplify weather descriptions
data['weather_description'] = data['weather_description'].str.lower()

# Print all unique categories in the weather_description column
unique_weather_descriptions = data['weather_description'].unique()
print('Unique Weather Descriptions:')
for description in unique_weather_descriptions:
    print(description)

# Define simplified mapping for weather descriptions
weather_simplification = {
    'thunderstorm': 'thunderstorm',
    'thunderstorm with drizzle': 'thunderstorm',
    'thunderstorm with heavy rain': 'thunderstorm',
    'thunderstorm with light drizzle': 'thunderstorm',
    'thunderstorm with light rain': 'thunderstorm',
    'thunderstorm with rain': 'thunderstorm',
    'proximity thunderstorm': 'thunderstorm',
    'proximity thunderstorm with drizzle': 'thunderstorm',
    'proximity thunderstorm with rain': 'thunderstorm',
    'squalls': 'thunderstorm',

    'drizzle': 'drizzle',
    'light intensity drizzle': 'drizzle',
    'shower drizzle': 'drizzle',
    'freezing rain': 'drizzle',
    'heavy intensity drizzle': 'drizzle',

    'fog': 'fog',
    'mist': 'fog',
    'haze': 'fog',

    'light rain': 'rain',
    'moderate rain': 'rain',
    'heavy intensity rain': 'rain',
    'proximity shower rain': 'rain',
    'very heavy rain': 'rain',
    'light rain and snow': 'rain/snow',
    'light intensity shower rain': 'rain/snow',

    'snow': 'snow',
    'heavy snow': 'snow',
    'shower snow': 'snow',
    'light shower snow': 'snow',

    'overcast clouds': 'cloudy',
    'scattered clouds': 'cloudy',
    'few clouds': 'cloudy',
    'broken clouds': 'cloudy',

    'sky is clear': 'clear',

    'smoke': 'smoke',
    'sleet': 'sleet'
}


# Apply the mapping to simplify weather descriptions
data['weather_description_simplified'] = data['weather_description'].map(weather_simplification)

# Print unique values in the simplified column to verify
unique_simplified_descriptions = data['weather_description_simplified'].unique()
print('Unique Simplified Weather Descriptions:')
for description in unique_simplified_descriptions:
    print(description)

"""### Exploratory Data Analysis"""

# Bar Chart Showing Weather Conditions
plt.figure(figsize=(12, 6))
weather_counts = data['weather_type'].value_counts()
weather_counts.plot(kind='bar', color='skyblue')
plt.xlabel('Weather Conditions')
plt.ylabel('Frequency')
plt.title('Weather Conditions Frequency')
plt.xticks(rotation=45)
plt.show()

# Distribution of Traffic Volume
plt.figure(figsize=(12, 6))
sns.histplot(data['traffic_volume'], bins=50, kde=True)
plt.xlabel('Traffic Volume')
plt.ylabel('Frequency')
plt.title('Distribution of Traffic Volume')
plt.show()

# Traffic volume by hour
data['hour'] = data['date_time'].dt.hour
plt.figure(figsize=(10, 6))
sns.boxplot(x='hour', y='traffic_volume', data=data)
plt.title('Traffic Volume by Hour')
plt.show()

# Traffic volume by day of the week
data['day_of_week'] = data['date_time'].dt.dayofweek
plt.figure(figsize=(10, 6))
sns.boxplot(x='day_of_week', y='traffic_volume', data=data)
plt.title('Traffic Volume by Day of the Week')
plt.show()

# Traffic volume by weather type
plt.figure(figsize=(15, 6))
sns.boxplot(x='weather_type', y='traffic_volume', data=data)
plt.title('Traffic Volume by Weather Type')
plt.xticks(rotation=90)
plt.show()

# Pairplot to see relationships between numerical features and traffic volume
numeric_features = ['air_pollution_index', 'humidity', 'wind_speed', 'visibility_in_miles', 'dew_point', 'temperature', 'rain_p_h', 'snow_p_h', 'traffic_volume']
sns.pairplot(data[numeric_features])
plt.suptitle('Pairplot of Numerical Features', y=1.02)
plt.show()

# Traffic volume vs temperature
plt.figure(figsize=(10, 6))
sns.scatterplot(x='temperature', y='traffic_volume', data=data)
plt.title('Traffic Volume vs Temperature')
plt.show()

# Traffic volume vs air pollution index
plt.figure(figsize=(10, 6))
sns.scatterplot(x='air_pollution_index', y='traffic_volume', data=data)
plt.title('Traffic Volume vs Air Pollution Index')
plt.show()

"""### Outlier Removal"""

# Handle outliers by capping to 1st and 99th percentile
def remove_outliers(df, column):
    lower_bound = df[column].quantile(0.01)
    upper_bound = df[column].quantile(0.99)
    df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)

for col in data.columns:
    if data[col].dtype != 'object':
        remove_outliers(data, col)

"""### Classification of Traffic Volume"""

# Define traffic volume categories
def categorize_traffic_volume(df):
    # Define thresholds
    high_threshold = df['traffic_volume'].quantile(0.75)
    low_threshold = df['traffic_volume'].quantile(0.25)

    # Categorize traffic volume
    df['traffic_volume_category'] = pd.cut(df['traffic_volume'],
                                           bins=[-float('inf'), low_threshold, high_threshold, float('inf')],
                                           labels=['Low', 'Medium', 'High'])

# Apply categorization
categorize_traffic_volume(data)

# Encode traffic volume categories
category_encoder = LabelEncoder()
data['traffic_volume_category_encoded'] = category_encoder.fit_transform(data['traffic_volume_category'])

# Print encoding mapping for traffic volume categories
print('Traffic Volume Category Encoding:')
for category, encoded in zip(category_encoder.classes_, category_encoder.transform(category_encoder.classes_)):
    print(f'{category} -> {encoded}')

# Plot bar chart of traffic volume categories
plt.figure(figsize=(12, 6))
category_counts = data['traffic_volume_category'].value_counts()
category_counts.plot(kind='bar', color='skyblue')

# Print ranges of counts for each category
category_ranges = data.groupby('traffic_volume_category')['traffic_volume'].agg([min, max])
print('Traffic Volume Ranges for Each Category:')
print(category_ranges)

# Check column names before dropping
print('Columns before dropping:', data.columns)

# Drop irrelevant coulumns
data.drop(['hour', 'day_of_week', 'traffic_volume', 'traffic_volume_category','weather_description','weather_description_simplified'], axis=1, inplace=True)

data.head()

"""### Label Encoding"""

# Encode categorical features
label_encoders = {}
for column in ['is_holiday', 'weather_type']:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le
    print(f'Encoding mapping for {column}:')
    mapping = dict(zip(le.classes_, le.transform(le.classes_)))
    print(mapping)

# Split features and target
X = data.drop(['date_time', 'traffic_volume_category_encoded','weather_description','weather_description_simplified'], axis=1, errors='ignore')
y = data['traffic_volume_category_encoded']

# Assuming X is a DataFrame
feature_names = X.columns
print("Features of X:")
print(feature_names)

"""### SMOTE Sampling"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42, sampling_strategy={0: 50000, 1: 50000, 2: 50000})
X_resampled, y_resampled = smote.fit_resample(X, y)

# Print samples after SMOTE
print("Samples after SMOTE:")
print("Features shape:", X_resampled.shape)
print("Target shape:", y_resampled.shape)
print("Class distribution after SMOTE:\n", pd.Series(y_resampled).value_counts())

"""### Machine Learning Models"""

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Convert test data to DataFrame
X_test_df = pd.DataFrame(X_test, columns=['is_holiday', 'air_pollution_index', 'humidity', 'wind_speed',
       'wind_direction', 'visibility_in_miles', 'dew_point', 'temperature',
       'rain_p_h', 'snow_p_h', 'clouds_all', 'weather_type'])
y_test_df = pd.DataFrame(y_test, columns=['traffic_volume_category_encoded'])

# Reset index before concatenation
X_test_df = X_test_df.reset_index(drop=True)
y_test_df = y_test_df.reset_index(drop=True)

# Concatenate
test_data_df = pd.concat([X_test_df, y_test_df], axis=1)
# Save to CSV
test_data_df.to_csv('test_data_traffic_volume.csv', index=False)

# XGBoost Model
xgb_model = XGBClassifier()
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
xgb_accuracy = accuracy_score(y_test, y_pred_xgb)
print(f'XGBoost Accuracy: {xgb_accuracy}')
print(f'XGBoost Classification Report:\n{classification_report(y_test, y_pred_xgb)}')

# Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)
print(f'Random Forest Accuracy: {rf_accuracy}')
print('Random Forest Classification Report:')
print(classification_report(y_test, y_pred_rf, target_names=category_encoder.classes_))

# Summary of Model Performance
print('Summary of Model Performance:')
print(f'XGBoost Accuracy: {xgb_accuracy}')
print(f'Random Forest Accuracy: {rf_accuracy}')

import pickle
# Save the best model (XGBoost) using pickle
with open('traffic_rf_model.pkl', 'wb') as model_file:
    pickle.dump(rf_model, model_file)
print('Best model saved as best_rf_model.pkl')

# Automatically download the saved model file
with open('traffic_rf_model.pkl', 'rb') as file:
    st.download_button(
        label="Download trained model",
        data=file,
        file_name="traffic_rf_model.pkl",
        mime="application/octet-stream",
    )

print('Best model saved as traffic_rf_model.pkl and ready for download')

